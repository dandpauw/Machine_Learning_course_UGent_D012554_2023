{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYXSmAOY4FGz"
   },
   "source": [
    "# BIAS <> VARIANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KsG9jpJs4FG_"
   },
   "source": [
    "So far we discussed several Machine Learning algorithms that each learn a predictive model from a training set using a certain model representation and a certain loss function. Machine Learning theory has shown that the prediction error that a model makes can be broken down into three parts: the **bias, the variance, and the irreducible part of the error**.\n",
    "\n",
    "To explain these concepts, we first need to first understand that the training set used to fit a model is typically a very small subset of all possible data points. This means that the generalization performance (the prediction error on unseen test set) depends, to a certain extend, on the particular choice of the training set sample (which is something we donâ€™t control). \n",
    "\n",
    "Imagine a dartboard where the goal is to hit the bullseye in the middle of the board. We throw several darts that try to hit the bullseye. The figure below shows several possible scenarios. \n",
    "\n",
    "<br/>\n",
    "<center>\n",
    "<img src=\"https://github.com/sdgroeve/Machine_Learning_course_UGent_D012554_data/raw/master/notebooks/6_ensemble_learning/bias-and-variance.jpg\" />\n",
    "</center>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_0YkB_Fi4FHC"
   },
   "source": [
    "In our context, the bullseye represents the true target of a test data point prediction (obtained with the true underlying model). Each dart represents the prediction for that test data point of a model trained on a different training set sample.\n",
    " \n",
    "**The error due to bias** is the difference between the expected prediction of our model and the correct value that we try to predict. By expected prediction we mean the average prediction of models trained on different training set samples. In the figure above we can see that for low bias models this average prediction (center point of the dart locations) is very close to the correct value. These models are able to predict the correct value. High bias models are not able to predict the correct value for any of the training set samples.\n",
    "\n",
    "**The error due to variance** is the variability of a model prediction for a given data point. The variance does not care about the correct value that we try to predict but instead looks at the variance of the predictions made by the different models. In the figure above we can see that low variance models tend to hit the same (or very close) location for each model, while predictions of high variance models are much more dependant on the actual choice of the training set sample.\n",
    "\n",
    "**The irreducible error** is a stochastic error that we cannot reduce by choosing a better model and is typically due to randomness or natural variability in a system (e.g. machine measurement errors). \n",
    "\n",
    "Examples of low variance, high bias algorithms are linear regression and logistic regression. They assume a relatively simple (not very flexible) model representation and as such are known to possess **low complexity**.\n",
    "\n",
    "Examples of high variance, low bias algorithms are (deep) neural networks and (deep) decision tree learning algorithms. They have a much more flexible model representation and as such are known to possess **high complexity**.\n",
    "\n",
    "This means that there is a **tradeoff** between the bias of a model and its variance. When minimizing one, the other will increase and *vice versa*. This is true because a model cannot possess low complexity and high complexity at the same time.  \n",
    "\n",
    "This tradeoff can be visualized as follows: \n",
    "\n",
    "<br/>\n",
    "<center>\n",
    "<img src=\"https://github.com/sdgroeve/Machine_Learning_course_UGent_D012554_data/raw/master/notebooks/6_ensemble_learning/biasvariance.png\" />\n",
    "</center>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mI3f1fz34FHH"
   },
   "source": [
    "The x-axis represents the complexity of a model. The y-axis shows the prediction error. As the model complexity increases the bias is reduced (the model is more flexible) but the variance will increase. The optimum model complexity is obtained when the increase in variance is equivalent to the reduction in bias.\n",
    "\n",
    "Note that the regularization term introduced for linear and logistic regression follows the same principle. High regularization means decreasing the complexity of the model representation by constraining the model parameters. This leads to higher bias and lower variance. Similarly, low regularization means increasing model complexity and results in lower bias with higher variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "6b_Bias-variance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
